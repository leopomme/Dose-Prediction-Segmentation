{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"include_colab_link":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/leopomme/Dose-Prediction-Segmentation/blob/main/notebook_DLMI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\n\nimport cv2\nimport albumentations as A\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim.lr_scheduler import StepLR\n\nimport time \nfrom tqdm import tqdm\nfrom glob import glob\nimport zipfile\nimport shutil\n","metadata":{"id":"59xiuIQZLCPP","execution":{"iopub.status.busy":"2023-03-24T11:30:31.413839Z","iopub.execute_input":"2023-03-24T11:30:31.414670Z","iopub.status.idle":"2023-03-24T11:30:36.992173Z","shell.execute_reply.started":"2023-03-24T11:30:31.414618Z","shell.execute_reply":"2023-03-24T11:30:36.990819Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/soniamartinot/MVA-Dose-Prediction.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LHRC5fH0LKcf","outputId":"c3f60c80-7635-4fb0-a74c-026ab1b1c7c2","execution":{"iopub.status.busy":"2023-03-24T11:31:34.176476Z","iopub.execute_input":"2023-03-24T11:31:34.177249Z","iopub.status.idle":"2023-03-24T11:32:29.777637Z","shell.execute_reply.started":"2023-03-24T11:31:34.177206Z","shell.execute_reply":"2023-03-24T11:32:29.776168Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'MVA-Dose-Prediction'...\nremote: Enumerating objects: 38724, done.\u001b[K\nremote: Total 38724 (delta 0), reused 0 (delta 0), pack-reused 38724\u001b[K\nReceiving objects: 100% (38724/38724), 77.02 MiB | 24.61 MiB/s, done.\nResolving deltas: 100% (30603/30603), done.\nUpdating files: 100% (39600/39600), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"transform_img = A.Compose([\n    A.OneOf([\n    A.GaussNoise(var_limit=(0, 10), p=1),\n    A.Blur(blur_limit=(3, 5), p=1),\n    ], p=1),\n])\n\ntransform_img_mask = A.Compose([\n    A.OneOf([\n        A.ElasticTransform(alpha=7, sigma=3, alpha_affine=3, p=1),\n        A.Rotate(limit=5, p=1),\n        A.VerticalFlip(p=1),\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=1),\n    ], p=1),\n])\n\ntrain_transforms = {\n    'img': transform_img,\n    'img_mask': transform_img_mask\n}\n\nval_transforms = None\n","metadata":{"id":"k7TofpC37Fah","execution":{"iopub.status.busy":"2023-03-24T11:32:29.781020Z","iopub.execute_input":"2023-03-24T11:32:29.781640Z","iopub.status.idle":"2023-03-24T11:32:29.793604Z","shell.execute_reply.started":"2023-03-24T11:32:29.781581Z","shell.execute_reply":"2023-03-24T11:32:29.792165Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class DoseDataset(torch.utils.data.Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.samples = os.listdir(data_path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        sample_path = self.data_path + os.sep + self.samples[idx]\n        ct_scan = np.load(sample_path + os.sep + 'ct.npy')\n        possible_dose_mask = np.load(sample_path + os.sep + 'possible_dose_mask.npy')\n        dose = np.load(sample_path + os.sep + 'dose.npy')\n        structure_masks = np.load(sample_path + os.sep + 'structure_masks.npy')\n\n        combined_masks = [possible_dose_mask] + [structure_masks[i] for i in range(structure_masks.shape[0])] + [dose]\n\n        if self.transform:\n            if 'img_mask' in self.transform:\n                transform_func = self.transform['img_mask']\n                augmentations = transform_func(image=ct_scan, masks=combined_masks)\n                ct_scan = augmentations['image']\n                combined_masks = augmentations['masks']\n            if 'img' in self.transform:\n                transform_func = self.transform['img']\n                augmentations = transform_func(image=ct_scan)\n                ct_scan = augmentations['image']\n\n        # Convert NumPy arrays to PyTorch tensors\n        ct_scan_tensor = torch.from_numpy(ct_scan).float().unsqueeze(0)\n        combined_masks_tensor = torch.from_numpy(np.stack(combined_masks[:-1], axis=0)).float()\n        dose_tensor = torch.from_numpy(combined_masks[-1]).float().unsqueeze(0)\n\n        # Normalize the ct_scan tensor\n        mean = ct_scan_tensor.mean()\n        std = ct_scan_tensor.std()\n        normalize = transforms.Normalize(mean=[mean], std=[std+1e-6])\n        ct_scan_tensor = normalize(ct_scan_tensor)\n\n        # Combine ct_scan_tensor and combined_masks_tensor\n        input_image = torch.cat([ct_scan_tensor, combined_masks_tensor], dim=0)\n\n        return {'input_image': input_image, 'dose': dose_tensor}\n","metadata":{"id":"oACxne0dLMio","execution":{"iopub.status.busy":"2023-03-24T11:32:29.795985Z","iopub.execute_input":"2023-03-24T11:32:29.796529Z","iopub.status.idle":"2023-03-24T11:32:30.786820Z","shell.execute_reply.started":"2023-03-24T11:32:29.796471Z","shell.execute_reply":"2023-03-24T11:32:30.785542Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class test_DoseDataset(torch.utils.data.Dataset):\n    def init(self, data_path, transform=None):\n        self.data_path = data_path\n        self.samples = os.listdir(data_path)\n        self.transform = transform\n\n    def len(self):\n        return len(self.samples)\n\n    def getitem(self, idx):\n        sample_path = self.data_path + os.sep + self.samples[idx]\n        ct_scan = np.load(sample_path + os.sep + 'ct.npy')\n        possible_dose_mask = np.load(sample_path + os.sep + 'possible_dose_mask.npy')\n        structure_masks = np.load(sample_path + os.sep + 'structure_masks.npy')\n        combined_masks = [possible_dose_mask] + [structure_masks[i] for i in range(structure_masks.shape[0])]\n\n        # Convert NumPy arrays to PyTorch tensors\n        ct_scan_tensor = torch.from_numpy(ct_scan).float().unsqueeze(0)\n        combined_masks_tensor = torch.from_numpy(np.stack(combined_masks, axis=0)).float()\n\n        # Normalize the ct_scan tensor\n        mean = ct_scan_tensor.mean()\n        std = ct_scan_tensor.std()\n        normalize = transforms.Normalize(mean=[mean], std=[std+1e-6])\n        ct_scan_tensor = normalize(ct_scan_tensor)\n\n        # Combine ct_scan_tensor and combined_masks_tensor\n        input_image = torch.cat([ct_scan_tensor, combined_masks_tensor], dim=0)\n\n        return {'sample_name': self.samples[idx], 'input_image': input_image}","metadata":{"id":"-w3tDPXwh4J3","execution":{"iopub.status.busy":"2023-03-24T11:32:30.789868Z","iopub.execute_input":"2023-03-24T11:32:30.790278Z","iopub.status.idle":"2023-03-24T11:32:31.040465Z","shell.execute_reply.started":"2023-03-24T11:32:30.790236Z","shell.execute_reply":"2023-03-24T11:32:31.038904Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset\n\ntrain_dir = \"./MVA-Dose-Prediction/train/\"\n\ntrain_dataset_original = DoseDataset(train_dir, transform=None)\ntrain_dataset_augmented = DoseDataset(train_dir, transform=train_transforms)\n\ntrain_dataset = ConcatDataset([train_dataset_original, train_dataset_augmented])\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, num_workers=2, shuffle=True, pin_memory=True)\nprint(len(train_dataloader))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Z-Xm8dNLZPG","outputId":"c1cbf311-71b1-4bb6-a0d2-99c5e9a7adfe","execution":{"iopub.status.busy":"2023-03-24T11:34:11.515541Z","iopub.execute_input":"2023-03-24T11:34:11.516983Z","iopub.status.idle":"2023-03-24T11:34:11.538631Z","shell.execute_reply.started":"2023-03-24T11:34:11.516920Z","shell.execute_reply":"2023-03-24T11:34:11.536906Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"488\n","output_type":"stream"}]},{"cell_type":"code","source":"val_dir = \"./MVA-Dose-Prediction/validation/\"\n\nval_dataset = DoseDataset(val_dir, transform=val_transforms)\nval_dataloader = DataLoader(val_dataset, batch_size=32, num_workers=2, shuffle=True, pin_memory=True)\nprint(len(val_dataloader))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gdAO-FbeNTnJ","outputId":"cec36390-fada-4a07-e026-f14fd1bedb68","execution":{"iopub.status.busy":"2023-03-24T11:34:11.874781Z","iopub.execute_input":"2023-03-24T11:34:11.875232Z","iopub.status.idle":"2023-03-24T11:34:11.885118Z","shell.execute_reply.started":"2023-03-24T11:34:11.875194Z","shell.execute_reply":"2023-03-24T11:34:11.883560Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"38\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dir = \"./MVA-Dose-Prediction/test\"\n\ntest_dataset = test_DoseDataset(test_dir)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, num_workers=2, shuffle=True, pin_memory=True)\nprint(len(test_dataloader))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyoQdyP3YxXo","outputId":"abc19792-7f4e-4046-84f6-50d26012719e","execution":{"iopub.status.busy":"2023-03-24T11:34:12.266808Z","iopub.execute_input":"2023-03-24T11:34:12.267719Z","iopub.status.idle":"2023-03-24T11:34:12.499035Z","shell.execute_reply.started":"2023-03-24T11:34:12.267664Z","shell.execute_reply":"2023-03-24T11:34:12.495919Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2688268266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./MVA-Dose-Prediction/test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_DoseDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/typing.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwds)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object.__new__() takes exactly one argument (the type to instantiate)"],"ename":"TypeError","evalue":"object.__new__() takes exactly one argument (the type to instantiate)","output_type":"error"}]},{"cell_type":"code","source":"class BigBasicBlock(nn.Module):\n    def __init__(self, in_channels: int, forward_expansion: int, out_channels: int):\n        super(BigBasicBlock, self).__init__()\n \n        self.conv1 = nn.Conv2d(in_channels, forward_expansion, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(forward_expansion)\n        self.conv2 = nn.Conv2d(forward_expansion, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = nn.ReLU()(out)\n        out = nn.Dropout(0.1)(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = nn.ReLU()(out)\n        out = nn.Dropout(0.125)(out)\n        return out\n\nclass BigUNet(nn.Module):\n    def __init__(self, in_channels=12, out_channels=1):\n        super(BigUNet, self).__init__()\n\n        # Downward pass\n\n        self.first = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n        )\n\n        self.down1 = nn.Sequential(\n            BigBasicBlock(64, 64, 128),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.down2 = nn.Sequential(\n            BigBasicBlock(128, 128, 256),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.down3 = nn.Sequential(\n            BigBasicBlock(256, 256, 512),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n\n        # Bottom block\n        self.bottom = BigBasicBlock(512, 512, 512)\n\n        # Upward pass\n        self.up3 = nn.Sequential(\n            nn.ConvTranspose2d(512+512, 256, kernel_size=2, stride=2),\n            BigBasicBlock(256, 256, 256),\n        )\n        self.up2 = nn.Sequential(\n            nn.ConvTranspose2d(256 + 256, 128, kernel_size=2, stride=2),\n            BigBasicBlock(128, 128, 128),\n        )\n        self.up1 = nn.Sequential(\n            nn.ConvTranspose2d(128 + 128, 64, kernel_size=2, stride=2),\n            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n        )\n\n        self.last = nn.Sequential(\n            nn.Conv2d(32 + in_channels, 16, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(8, 4, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(4, 2, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(2, 1, kernel_size=3, stride=1, padding=1)  \n        )\n\n    def forward(self, x):\n      # Downward pass\n      x_initial = self.first(x)\n      x1 = self.down1(x_initial)\n      x2 = self.down2(x1)\n      x3 = self.down3(x2)\n\n      # Bottom block\n      bottom = self.bottom(x3)\n\n      # Upward pass\n      y3 = self.up3(torch.cat((bottom, x3), dim=1))\n      y2 = self.up2(torch.cat((y3, x2), dim=1))\n      y1 = self.up1(torch.cat((y2, x1), dim=1))\n      \n      # Concatenate input layer\n      y_concat = torch.cat((y1, x), dim=1)\n      y = self.last(y_concat)\n      y = nn.ReLU()(y)\n\n      y = y * x[:, 1:2, :, :]\n\n      return y\n\n","metadata":{"id":"KTTDlkFpiGnV","execution":{"iopub.status.busy":"2023-03-24T11:34:12.670571Z","iopub.execute_input":"2023-03-24T11:34:12.672867Z","iopub.status.idle":"2023-03-24T11:34:12.698959Z","shell.execute_reply.started":"2023-03-24T11:34:12.672799Z","shell.execute_reply":"2023-03-24T11:34:12.697661Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Create a training loop\ndef train(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for i, data in enumerate(dataloader, 0):\n        inputs, labels = data['input_image'].to(device), data['dose'].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    return running_loss / (i + 1)\n\n# Create a validation loop\ndef validate(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    with torch.no_grad():\n        for i, data in enumerate(dataloader, 0):\n            inputs, labels = data['input_image'].to(device), data['dose'].to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n    return running_loss / (i + 1)\n","metadata":{"id":"wCTO3vKnLCPd","execution":{"iopub.status.busy":"2023-03-24T11:34:13.523497Z","iopub.execute_input":"2023-03-24T11:34:13.523989Z","iopub.status.idle":"2023-03-24T11:34:13.534542Z","shell.execute_reply.started":"2023-03-24T11:34:13.523951Z","shell.execute_reply":"2023-03-24T11:34:13.533141Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n  def __init__(self, patience = 5):\n    self.patience = patience\n    self.best_val_loss = None\n    self.counter = 0\n    self.early_stop = False\n\n  def __call__(self, val_loss):\n    if self.best_val_loss is None or val_loss < self.best_val_loss:\n        self.best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_trained_model.pth')\n        print('model saved')\n        self.counter = 0\n    else:\n        self.counter += 1","metadata":{"id":"W4F91_0xMK6M","execution":{"iopub.status.busy":"2023-03-24T11:34:15.414931Z","iopub.execute_input":"2023-03-24T11:34:15.415699Z","iopub.status.idle":"2023-03-24T11:34:15.422643Z","shell.execute_reply.started":"2023-03-24T11:34:15.415654Z","shell.execute_reply":"2023-03-24T11:34:15.421412Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Train the model and evaluate its performance on the validation set\nnum_epochs = 200\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel = BigUNet().to(device)\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(model.parameters(), lr=0.00005)\n#scheduler = StepLR(optimizer, step_size=10, gamma=-0.5)\nearly_stopping = EarlyStopping(patience = 100)\n\ntrain_loss_list = []\nval_loss_list = []\n\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    epoch_time = time.time()\n\n    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n    val_loss = validate(model, val_dataloader, criterion, device)\n\n    elapsed_time = time.time() - epoch_time\n    total_time = time.time() - start_time\n    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Epoch time: {elapsed_time/60:.2f}min, Total time: {total_time/60:.2f}min')\n\n    # Update the learning rate using the scheduler\n    #scheduler.step()\n\n    train_loss_list.append(train_loss)\n    val_loss_list.append(val_loss)\n    \n    early_stopping(val_loss)\n    if early_stopping.counter >= early_stopping.patience:\n      print(f'Early stopping at epoch {epoch + 1}. Best epoch: {epoch + 1 - early_stopping.counter} with Val Loss: {early_stopping.best_val_loss:.4f}')\n      break","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41jaPhkTLCPd","outputId":"29ab30f2-64bf-46de-b7b3-40155335faf8","execution":{"iopub.status.busy":"2023-03-23T03:41:23.019383Z","iopub.execute_input":"2023-03-23T03:41:23.019806Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda\nEpoch 1/200, Train Loss: 0.8370, Val Loss: 0.4836, Epoch time: 3.44min, Total time: 3.44min\nmodel saved\nEpoch 2/200, Train Loss: 0.4857, Val Loss: 0.5346, Epoch time: 3.33min, Total time: 6.77min\nEpoch 3/200, Train Loss: 0.4602, Val Loss: 0.4283, Epoch time: 3.39min, Total time: 10.16min\nmodel saved\nEpoch 4/200, Train Loss: 0.4370, Val Loss: 0.4538, Epoch time: 3.38min, Total time: 13.54min\nEpoch 5/200, Train Loss: 0.4248, Val Loss: 0.4088, Epoch time: 3.39min, Total time: 16.93min\nmodel saved\nEpoch 6/200, Train Loss: 0.4094, Val Loss: 0.4610, Epoch time: 3.33min, Total time: 20.27min\nEpoch 7/200, Train Loss: 0.3972, Val Loss: 0.4035, Epoch time: 3.35min, Total time: 23.62min\nmodel saved\nEpoch 8/200, Train Loss: 0.3845, Val Loss: 0.4042, Epoch time: 3.44min, Total time: 27.05min\nEpoch 9/200, Train Loss: 0.3771, Val Loss: 0.4008, Epoch time: 3.39min, Total time: 30.45min\nmodel saved\nEpoch 10/200, Train Loss: 0.3704, Val Loss: 0.3972, Epoch time: 3.35min, Total time: 33.80min\nmodel saved\nEpoch 11/200, Train Loss: 0.3605, Val Loss: 0.4011, Epoch time: 3.42min, Total time: 37.22min\nEpoch 12/200, Train Loss: 0.3539, Val Loss: 0.3964, Epoch time: 3.37min, Total time: 40.59min\nmodel saved\nEpoch 13/200, Train Loss: 0.3456, Val Loss: 0.3980, Epoch time: 3.44min, Total time: 44.03min\nEpoch 14/200, Train Loss: 0.3398, Val Loss: 0.3887, Epoch time: 3.40min, Total time: 47.43min\nmodel saved\nEpoch 15/200, Train Loss: 0.3346, Val Loss: 0.4154, Epoch time: 3.51min, Total time: 50.94min\nEpoch 16/200, Train Loss: 0.3273, Val Loss: 0.3943, Epoch time: 3.45min, Total time: 54.39min\nEpoch 17/200, Train Loss: 0.3224, Val Loss: 0.3870, Epoch time: 3.50min, Total time: 57.89min\nmodel saved\nEpoch 18/200, Train Loss: 0.3154, Val Loss: 0.3904, Epoch time: 3.37min, Total time: 61.26min\nEpoch 19/200, Train Loss: 0.3116, Val Loss: 0.3906, Epoch time: 3.48min, Total time: 64.74min\nEpoch 20/200, Train Loss: 0.3065, Val Loss: 0.3830, Epoch time: 3.42min, Total time: 68.17min\nmodel saved\nEpoch 21/200, Train Loss: 0.3012, Val Loss: 0.3865, Epoch time: 3.44min, Total time: 71.61min\nEpoch 22/200, Train Loss: 0.2954, Val Loss: 0.3820, Epoch time: 3.45min, Total time: 75.06min\nmodel saved\nEpoch 23/200, Train Loss: 0.2919, Val Loss: 0.3801, Epoch time: 3.49min, Total time: 78.55min\nmodel saved\nEpoch 24/200, Train Loss: 0.2895, Val Loss: 0.3999, Epoch time: 3.45min, Total time: 82.00min\nEpoch 25/200, Train Loss: 0.2865, Val Loss: 0.3795, Epoch time: 3.46min, Total time: 85.46min\nmodel saved\nEpoch 26/200, Train Loss: 0.2812, Val Loss: 0.3754, Epoch time: 3.41min, Total time: 88.87min\nmodel saved\nEpoch 27/200, Train Loss: 0.2786, Val Loss: 0.3804, Epoch time: 3.34min, Total time: 92.22min\nEpoch 28/200, Train Loss: 0.2739, Val Loss: 0.3798, Epoch time: 3.40min, Total time: 95.62min\nEpoch 29/200, Train Loss: 0.2729, Val Loss: 0.3857, Epoch time: 3.38min, Total time: 99.00min\nEpoch 30/200, Train Loss: 0.2691, Val Loss: 0.3798, Epoch time: 3.40min, Total time: 102.40min\nEpoch 31/200, Train Loss: 0.2659, Val Loss: 0.3802, Epoch time: 3.42min, Total time: 105.82min\nEpoch 32/200, Train Loss: 0.2656, Val Loss: 0.3851, Epoch time: 3.37min, Total time: 109.20min\nEpoch 33/200, Train Loss: 0.2606, Val Loss: 0.3881, Epoch time: 3.35min, Total time: 112.54min\nEpoch 34/200, Train Loss: 0.2597, Val Loss: 0.3834, Epoch time: 3.38min, Total time: 115.92min\nEpoch 35/200, Train Loss: 0.2578, Val Loss: 0.3782, Epoch time: 3.39min, Total time: 119.30min\nEpoch 36/200, Train Loss: 0.2554, Val Loss: 0.3783, Epoch time: 3.37min, Total time: 122.67min\nEpoch 37/200, Train Loss: 0.2509, Val Loss: 0.3816, Epoch time: 3.37min, Total time: 126.04min\nEpoch 38/200, Train Loss: 0.2505, Val Loss: 0.3839, Epoch time: 3.34min, Total time: 129.39min\nEpoch 39/200, Train Loss: 0.2487, Val Loss: 0.3832, Epoch time: 3.38min, Total time: 132.76min\nEpoch 40/200, Train Loss: 0.2479, Val Loss: 0.3766, Epoch time: 3.36min, Total time: 136.13min\nEpoch 41/200, Train Loss: 0.2457, Val Loss: 0.3826, Epoch time: 3.38min, Total time: 139.50min\nEpoch 42/200, Train Loss: 0.2421, Val Loss: 0.3761, Epoch time: 3.34min, Total time: 142.84min\nEpoch 43/200, Train Loss: 0.2410, Val Loss: 0.3813, Epoch time: 3.35min, Total time: 146.19min\nEpoch 44/200, Train Loss: 0.2407, Val Loss: 0.3775, Epoch time: 3.37min, Total time: 149.56min\nEpoch 45/200, Train Loss: 0.2384, Val Loss: 0.3717, Epoch time: 3.57min, Total time: 153.13min\nmodel saved\nEpoch 46/200, Train Loss: 0.2378, Val Loss: 0.3770, Epoch time: 3.39min, Total time: 156.52min\nEpoch 47/200, Train Loss: 0.2356, Val Loss: 0.3778, Epoch time: 3.36min, Total time: 159.88min\nEpoch 48/200, Train Loss: 0.2338, Val Loss: 0.3757, Epoch time: 3.36min, Total time: 163.24min\nEpoch 49/200, Train Loss: 0.2310, Val Loss: 0.3781, Epoch time: 3.32min, Total time: 166.56min\nEpoch 50/200, Train Loss: 0.2295, Val Loss: 0.3750, Epoch time: 3.40min, Total time: 169.96min\nEpoch 51/200, Train Loss: 0.2288, Val Loss: 0.3752, Epoch time: 3.41min, Total time: 173.37min\nEpoch 52/200, Train Loss: 0.2274, Val Loss: 0.3723, Epoch time: 3.40min, Total time: 176.77min\nEpoch 53/200, Train Loss: 0.2290, Val Loss: 0.3762, Epoch time: 3.34min, Total time: 180.11min\nEpoch 54/200, Train Loss: 0.2255, Val Loss: 0.3761, Epoch time: 3.31min, Total time: 183.42min\nEpoch 55/200, Train Loss: 0.2237, Val Loss: 0.3799, Epoch time: 3.34min, Total time: 186.76min\nEpoch 56/200, Train Loss: 0.2216, Val Loss: 0.3788, Epoch time: 3.35min, Total time: 190.11min\nEpoch 57/200, Train Loss: 0.2212, Val Loss: 0.3792, Epoch time: 3.33min, Total time: 193.44min\nEpoch 58/200, Train Loss: 0.2223, Val Loss: 0.3789, Epoch time: 3.33min, Total time: 196.76min\nEpoch 59/200, Train Loss: 0.2194, Val Loss: 0.3751, Epoch time: 3.32min, Total time: 200.08min\nEpoch 60/200, Train Loss: 0.2185, Val Loss: 0.3784, Epoch time: 3.33min, Total time: 203.41min\nEpoch 61/200, Train Loss: 0.2176, Val Loss: 0.3757, Epoch time: 3.38min, Total time: 206.79min\nEpoch 62/200, Train Loss: 0.2158, Val Loss: 0.3778, Epoch time: 3.35min, Total time: 210.14min\nEpoch 63/200, Train Loss: 0.2159, Val Loss: 0.3765, Epoch time: 3.34min, Total time: 213.48min\nEpoch 64/200, Train Loss: 0.2139, Val Loss: 0.3780, Epoch time: 3.33min, Total time: 216.81min\nEpoch 65/200, Train Loss: 0.2154, Val Loss: 0.3786, Epoch time: 3.35min, Total time: 220.16min\nEpoch 66/200, Train Loss: 0.2128, Val Loss: 0.3743, Epoch time: 3.30min, Total time: 223.46min\nEpoch 67/200, Train Loss: 0.2118, Val Loss: 0.3749, Epoch time: 3.33min, Total time: 226.79min\nEpoch 68/200, Train Loss: 0.2108, Val Loss: 0.3755, Epoch time: 3.33min, Total time: 230.12min\nEpoch 69/200, Train Loss: 0.2115, Val Loss: 0.3759, Epoch time: 3.28min, Total time: 233.40min\nEpoch 70/200, Train Loss: 0.2104, Val Loss: 0.3748, Epoch time: 3.29min, Total time: 236.69min\nEpoch 71/200, Train Loss: 0.2074, Val Loss: 0.3791, Epoch time: 3.29min, Total time: 239.98min\nEpoch 72/200, Train Loss: 0.2072, Val Loss: 0.3780, Epoch time: 3.28min, Total time: 243.26min\nEpoch 73/200, Train Loss: 0.2063, Val Loss: 0.3807, Epoch time: 3.31min, Total time: 246.57min\nEpoch 74/200, Train Loss: 0.2060, Val Loss: 0.3766, Epoch time: 3.27min, Total time: 249.84min\nEpoch 75/200, Train Loss: 0.2058, Val Loss: 0.3721, Epoch time: 3.27min, Total time: 253.11min\nEpoch 76/200, Train Loss: 0.2042, Val Loss: 0.3745, Epoch time: 3.28min, Total time: 256.40min\nEpoch 77/200, Train Loss: 0.2032, Val Loss: 0.3726, Epoch time: 3.28min, Total time: 259.67min\nEpoch 78/200, Train Loss: 0.2041, Val Loss: 0.3756, Epoch time: 3.30min, Total time: 262.97min\nEpoch 79/200, Train Loss: 0.2020, Val Loss: 0.3740, Epoch time: 3.34min, Total time: 266.31min\nEpoch 80/200, Train Loss: 0.2026, Val Loss: 0.3755, Epoch time: 3.30min, Total time: 269.61min\nEpoch 81/200, Train Loss: 0.2019, Val Loss: 0.3737, Epoch time: 3.29min, Total time: 272.90min\nEpoch 82/200, Train Loss: 0.2009, Val Loss: 0.3780, Epoch time: 3.30min, Total time: 276.20min\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrain_loss = [0.7158, 0.4868, 0.4590, 0.4452, 0.4205, 0.4130, 0.4000, 0.3829, 0.3771, 0.3678, 0.3405, 0.3356, 0.3322, 0.3286, 0.3276, 0.3236, 0.3219, 0.3205, 0.3180, 0.3159, 0.3120, 0.3116, 0.3101, 0.3099, 0.3096, 0.3107, 0.3083]\nval_loss = [0.4559, 0.4534, 0.5230, 0.4157, 0.4295, 0.4657, 0.4076, 0.3997, 0.4892, 0.3892, 0.3851, 0.3806, 0.3808, 0.3800, 0.3827, 0.3816, 0.3799, 0.3838, 0.3835, 0.3839, 0.3823, 0.3838, 0.3810, 0.3816, 0.3798, 0.3818, 0.3806]\n\nepochs = range(0, len(train_loss))\n\nplt.plot( epochs ,train_loss)\nplt.plot(epochs , val_loss)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(len(train_loss_list)), train_loss_list, label='Training Loss')\nplt.plot(range(len(val_loss_list)), val_loss_list, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show","metadata":{"id":"qfshmoP3lnbs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BigUnet -> train: val: 0.9933\n\n\nBigUnet with dropout -> train: val: 1.001\n\n","metadata":{"id":"vN7gYQ0bCLJ3"}},{"cell_type":"code","source":"# load the trained model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BigUNet().to(device)\n# Load the trained model\nmodel.load_state_dict(torch.load(\"/kaggle/input/model/best_trained_model.pth\"))","metadata":{"id":"FHYyRJM8o4XT","execution":{"iopub.status.busy":"2023-03-24T11:35:00.960468Z","iopub.execute_input":"2023-03-24T11:35:00.961331Z","iopub.status.idle":"2023-03-24T11:35:01.179876Z","shell.execute_reply.started":"2023-03-24T11:35:00.961276Z","shell.execute_reply":"2023-03-24T11:35:01.178298Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3686364346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/model/best_trained_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         loaded_storages[key] = torch.storage.TypedStorage(\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m             dtype=dtype)\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    167\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."],"ename":"RuntimeError","evalue":"Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport zipfile\n\ndef predict(model, dataloader, device):\n    model.eval()\n    predictions = {}\n    with torch.no_grad():\n        for data in dataloader:\n            sample_name = data['sample_name']\n            input_image = data['input_image'].to(device)\n            output = model(input_image)\n            output_np = output.cpu().numpy()\n            predictions[sample_name] = output_np\n    return predictions\n\n# Generate predictions\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\npredictions = predict(model, test_dataloader, device)\n\n# Save predictions as .npy files\npred_dir = \"predictions\"\nos.makedirs(pred_dir, exist_ok=True)\n\nfor sample_name, pred in predictions.items():\n    pred_file = os.path.join(pred_dir, f\"{sample_name}.npy\")\n    np.save(pred_file, pred)\n\n# Create a .zip archive containing all .npy files\nwith zipfile.ZipFile(\"submission.zip\", \"w\") as archive:\n    for pred_file in os.listdir(pred_dir):\n        archive.write(os.path.join(pred_dir, pred_file), pred_file)","metadata":{"id":"gnWh1PyMeKb6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DenseBlock(nn.Module):\n    def __init__(self, in_channels, growth_rate, num_layers, dropout_rate, l2):\n        super(DenseBlock, self).__init__()\n        self.layers = nn.ModuleList([\n            nn.Sequential(\n                nn.BatchNorm2d(in_channels + growth_rate * i),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels + growth_rate * i, growth_rate, 3, padding=1, bias=False),\n                nn.Dropout(dropout_rate) if dropout_rate else nn.Identity()\n            )\n            for i in range(num_layers)\n        ])\n    \n    def forward(self, x):\n        for layer in self.layers:\n            out = layer(x)\n            x = torch.cat([x, out], dim=1)\n        return x\n\nclass TransitionDown(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout_rate):\n        super(TransitionDown, self).__init__()\n        self.layers = nn.Sequential(\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.Dropout(dropout_rate) if dropout_rate else nn.Identity(),\n            nn.AvgPool2d(2)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass DenseUNet(nn.Module):\n    def __init__(self, input_shape, dropout_rate=None, l2=0.00000001, activation='relu', lr=0.0001, print_summary=False):\n        super(DenseUNet, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(input_shape[2], 32, 3, padding=1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True)\n        )\n\n        growth_rates = [8, 16, 32, 64]\n        self.dense_blocks = nn.ModuleList([DenseBlock(32 + sum(growth_rates[:i]), growth_rate, 4, dropout_rate, l2) for i, growth_rate in enumerate(growth_rates)])\n        self.transition_downs = nn.ModuleList([TransitionDown(32 + sum(growth_rates[:i + 1]), 32 + sum(growth_rates[:i + 1]), dropout_rate) for i in range(len(growth_rates) - 1)])\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(800, 512, 3, stride=2, padding=1, output_padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 256, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            DenseBlock(256, 64, 4, dropout_rate, l2),\n            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 128, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            DenseBlock(128, 32, 4, dropout_rate, l2),\n            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 64, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            DenseBlock(64, 16, 4, dropout_rate, l2),\n            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 32, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            DenseBlock(32, 8, 4, dropout_rate, l2)\n            )\n\n        self.output = nn.Conv2d(80, 1, 1, bias=True)\n            \n        self.print_summary = print_summary\n\n        def forward(self, x):\n            x1 = self.encoder(x)\n            x = x1\n\n            for dense_block, transition_down in zip(self.dense_blocks[:-1], self.transition_downs):\n                x = dense_block(x)\n                x = transition_down(x)\n\n            x = self.dense_blocks[-1](x)\n\n            x = self.decoder(x)\n            x = torch.cat([x, x1], dim=1)\n            x = self.output(x)\n            \n            return x\n\n","metadata":{"id":"VGXygExvLCPg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}